{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMr11o8GMkZ7LEu/uXrJE42",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NHleza/Wek--5/blob/main/Untitled4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2I4EsrLqm5Bn"
      },
      "outputs": [],
      "source": [
        "# AI Development Workflow Coding Example\n",
        "# Problem: Predicting Student Dropout Rates\n",
        "# Model: Random Forest Classifier\n",
        "# Dataset: Hypothetical student data with features like grades, attendance, socioeconomic info\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import f1_score, roc_auc_score, confusion_matrix, precision_score, recall_score\n",
        "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# --- Step 1: Load Data ---\n",
        "# For demonstration, create a synthetic dataset\n",
        "np.random.seed(42)\n",
        "data_size = 1000\n",
        "\n",
        "data = pd.DataFrame({\n",
        "    'grade_avg': np.random.uniform(50, 100, data_size),\n",
        "    'attendance_rate': np.random.uniform(0.5, 1.0, data_size),\n",
        "    'parent_education': np.random.choice(['High School', 'Bachelor', 'Master', 'None'], data_size),\n",
        "    'family_income': np.random.uniform(20000, 100000, data_size),\n",
        "    'disciplinary_actions': np.random.poisson(1, data_size),\n",
        "    'dropout': np.random.choice([0, 1], data_size, p=[0.85, 0.15])  # 15% dropout rate\n",
        "})\n",
        "\n",
        "# Introduce some missing values\n",
        "for col in ['grade_avg', 'attendance_rate', 'family_income']:\n",
        "    data.loc[data.sample(frac=0.1).index, col] = np.nan\n",
        "\n",
        "# --- Step 2: Preprocessing ---\n",
        "\n",
        "# Separate features and target\n",
        "X = data.drop('dropout', axis=1)\n",
        "y = data['dropout']\n",
        "\n",
        "# Impute missing numerical values with median\n",
        "num_cols = ['grade_avg', 'attendance_rate', 'family_income', 'disciplinary_actions']\n",
        "num_imputer = SimpleImputer(strategy='median')\n",
        "X[num_cols] = num_imputer.fit_transform(X[num_cols])\n",
        "\n",
        "# Encode categorical variables\n",
        "cat_cols = ['parent_education']\n",
        "encoder = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
        "encoded_cat = encoder.fit_transform(X[cat_cols])\n",
        "encoded_cat_df = pd.DataFrame(encoded_cat, columns=encoder.get_feature_names_out(cat_cols))\n",
        "X = X.drop(cat_cols, axis=1)\n",
        "X = pd.concat([X.reset_index(drop=True), encoded_cat_df.reset_index(drop=True)], axis=1)\n",
        "\n",
        "# Normalize numerical features\n",
        "scaler = MinMaxScaler()\n",
        "X[num_cols] = scaler.fit_transform(X[num_cols])\n",
        "\n",
        "# --- Step 3: Split Data ---\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n",
        "\n",
        "# --- Step 4: Model Development and Hyperparameter Tuning ---\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200],\n",
        "    'max_depth': [5, 10, None]\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(rf, param_grid, cv=3, scoring='f1', n_jobs=-1)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best parameters:\", grid_search.best_params_)\n",
        "\n",
        "# --- Step 5: Evaluation on Validation Set ---\n",
        "y_val_pred = grid_search.predict(X_val)\n",
        "f1 = f1_score(y_val, y_val_pred)\n",
        "roc_auc = roc_auc_score(y_val, grid_search.predict_proba(X_val)[:,1])\n",
        "\n",
        "print(f\"Validation F1 Score: {f1:.4f}\")\n",
        "print(f\"Validation ROC-AUC: {roc_auc:.4f}\")\n",
        "\n",
        "# --- Step 6: Final Evaluation on Test Set ---\n",
        "y_test_pred = grid_search.predict(X_test)\n",
        "f1_test = f1_score(y_test, y_test_pred)\n",
        "roc_auc_test = roc_auc_score(y_test, grid_search.predict_proba(X_test)[:,1])\n",
        "\n",
        "print(f\"Test F1 Score: {f1_test:.4f}\")\n",
        "print(f\"Test ROC-AUC: {roc_auc_test:.4f}\")\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_test, y_test_pred)\n",
        "print(\"Confusion Matrix:\\n\", cm)\n",
        "\n",
        "# Precision and Recall\n",
        "precision = precision_score(y_test, y_test_pred)\n",
        "recall = recall_score(y_test, y_test_pred)\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "\n",
        "# --- Step 7: Save Model (Optional) ---\n",
        "import joblib\n",
        "joblib.dump(grid_search.best_estimator_, 'student_dropout_rf_model.joblib')\n",
        "\n",
       
      ]
    }
  ]
}
